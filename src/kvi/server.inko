import kvi.config (Config)
import kvi.logger (Logger)
import kvi.map (Hasher, Key, Map)
import kvi.resp (Error, Stream)
import std.clone (Clone)
import std.net.socket (TcpClient, TcpServer)
import std.signal (Signal, Signals)

# Starts the various server processes.
#
# This method blocks the calling process until the database is shut down.
fn start(config: Config) -> Result[Nil, String] {
  let logger = Logger.new(config.log_level)
  let accepters = config.accepters.to_int
  let num_shards = config.shards.to_int
  let port = config.port.to_int
  let shards = Shards.new(num_shards)
  let hasher = Hasher.new

  logger.info('starting with ${accepters} accepters and ${num_shards} shards')

  let sockets = try config.ips.iter.try_reduce([], fn (socks, ip) {
    logger.info('listening on ${ip}:${port}')

    let sock = recover {
      match TcpServer.new(ip, port) {
        case Ok(v) -> v
        case Error(e) -> throw 'failed to bind to ${ip}:${port}: ${e}'
      }
    }

    for _ in 0.until(accepters) {
      let sock = recover {
        match sock.try_clone {
          case Ok(v) -> v
          case Error(e) -> throw 'failed to clone a accepter socket: ${e}'
        }
      }

      let logger = recover logger.clone
      let shards = recover shards.clone

      Accepter.new(logger, sock, shards, hasher.clone).run
    }

    socks.push(recover sock)
    Result.Ok(socks)
  })

  # These are the signals we're explicitly interested in. All other signals
  # are ignored.
  let signals = Signals.new

  signals.add(Signal.Interrupt)
  signals.add(Signal.Terminate)
  signals.add(Signal.Quit)

  loop {
    match signals.wait {
      case Interrupt or Terminate -> {
        logger.info('shutting down gracefully')
        break
      }
      case _ -> return Result.Ok(nil)
    }
  }

  # Shutting down the sockets wakes up any sleeping accepter processes,
  # allowing them to shut down cleanly without accepting new connections.
  for s in sockets { let _ = s.socket.shutdown }

  # Wait for any pending log data to be written, otherwise we might lose that
  # information.
  logger.flush
  Result.Ok(nil)
}

# A process that waits for incoming connections on a TCP socket.
type async Accepter {
  # The logger to use for log messages produced by this process and the
  # processes for each individual connection.
  let @logger: Logger

  # The socket to listen on for incoming connections.
  let @socket: TcpServer

  # The list of shards to give to each connection.
  let @shards: Shards

  # The hasher to give to each connection.
  let @hasher: Hasher

  fn static new(
    logger: uni Logger,
    socket: uni TcpServer,
    shards: uni Shards,
    hasher: Hasher,
  ) -> Accepter {
    Accepter(logger: logger, socket: socket, shards: shards, hasher: hasher)
  }

  fn async mut run {
    loop {
      let stream = recover {
        match @socket.accept {
          case Ok(v) -> Stream.new(v)
          # We may encounter this error if the server is being shut down in
          # response to e.g. a SIGINT signal.
          case Error(InvalidArgument) -> break
          case Error(e) -> {
            @logger.error('failed to accept a new connection: ${e}')
            next
          }
        }
      }

      # Getting the peer address may fail if the socket disconnects at this
      # point. As such we'll just ignore such connections.
      let label = match stream.stream.peer_address {
        case Ok({ @ip = ip, @port = port }) -> '${ip}:${port}'
        case Error(_) -> next
      }

      let logger = recover @logger.with_label(label)
      let shards = recover @shards.clone

      logger.info('established new connection')
      Connection.new(logger, shards, @hasher.clone).resume(stream)
    }
  }
}

# A process that represents a single connection between the server and a client.
type async Connection {
  # The logger dedicated to this connection.
  let @logger: Logger

  # The list of shards to use for operations.
  let @shards: Shards

  # The hasher to use for generating hash codes and determining what shard to
  # use for a certain key.
  let @hasher: Hasher

  fn static new(
    logger: uni Logger,
    shards: uni Shards,
    hasher: Hasher,
  ) -> Connection {
    Connection(logger: logger, shards: shards, hasher: hasher)
  }

  # Starts or resumes a connection.
  #
  # This message reads the next command in the pipeline (automatically starting
  # a new one if needed) and handles it accordingly.
  fn async mut resume(stream: uni Stream[TcpClient]) {
    loop {
      let cmd = match stream.read_pipeline_command {
        case Ok(v) -> v
        case Error(e) -> return error(stream, e)
      }

      match cmd {
        case Hello -> {
          match stream.read_pipeline_string {
            case Ok('3') -> {
              match stream.write_hello_response {
                case Ok(_) -> {}
                case Error(e) -> return error(stream, e)
              }
            }
            case Ok(v) -> {
              let e = recover Error.Hard("unsupported protocol version '${v}'")

              return error(stream, e)
            }
            case Error(e) -> return error(stream, e)
          }

          # HELLO commands are handled by this process, so we don't break out of
          # the loop and instead wait for another command on the next iteration.
        }
        case Get -> {
          match stream.read_key(@hasher) {
            case Ok(k) -> @shards.select(@hasher, k.hash).get(self, k, stream)
            case Error(e) -> error(stream, e)
          }

          return
        }
        case Set -> {
          match stream.read_key(@hasher) {
            case Ok(k) -> @shards.select(@hasher, k.hash).set(self, k, stream)
            case Error(e) -> error(stream, e)
          }

          return
        }
        case Delete -> {
          match stream.read_key(@hasher) {
            case Ok(k) -> {
              @shards.select(@hasher, k.hash).delete(self, k, stream)
            }
            case Error(e) -> error(stream, e)
          }

          return
        }
        case Keys -> {
          let shards = recover @shards.to_array
          let keys = recover []
          let shard = shards.pop.get

          shard.keys(self, stream, shards, keys)
          return
        }
      }
    }
  }

  # Writes all keys produced by the shards to the stream.
  #
  # This message is to be sent by the last shard to produce its set of keys.
  fn async mut write_keys(
    stream: uni Stream[TcpClient],
    keys: uni Array[ByteArray],
  ) {
    let keys = recover keys

    match stream.write_array(keys.size) {
      case Ok(_) -> {}
      case Error(e) -> return error(stream, e)
    }

    for key in keys {
      match stream.write_bulk_string(key) {
        case Ok(_) -> {}
        case Error(e) -> return error(stream, e)
      }
    }

    resume(stream)
  }

  # Handles an error produced by a stream.
  #
  # Error handling is generally the same across the different parts of the
  # database: log a message, maybe report the message to the client, and maybe
  # disconnect the connection. We centralize this logic here such that we don't
  # have to duplicate it in various places.
  #
  # For IO and hard errors, this method terminates the connection simply by not
  # scheduling any new messages. For soft errors the connection is resumed after
  # reporting the error.
  fn async mut error(stream: uni Stream[TcpClient], error: uni Error) {
    match error {
      # IO errors result in a disconnect because we can't meaningfully continue
      # after encountering one (the client might be disconnected, the state
      # might be messed up, etc).
      case ReadWrite(e) -> @logger.debug(e.to_string)
      case Closed -> @logger.info('client disconnected')
      case Hard(e) -> {
        let _ = stream.write_error(e)

        @logger.error(e)
      }
      case Soft(e) -> {
        let _ = stream.write_error(e)

        @logger.error(e)
        resume(stream)
      }
    }
  }
}

# A collection of shards to use for storing data.
type inline Shards {
  let @shards: Array[Shard]

  # Returns a list of `size` shards.
  fn static new(size: Int) -> Shards {
    let shards = Array.with_capacity(size)

    for _ in 0.until(size) { shards.push(Shard.new) }

    Shards(shards)
  }

  # Returns the `Shard` to use for the given hash code.
  #
  # The shard is chosen using [rendezvous
  # hashing](https://en.wikipedia.org/wiki/Rendezvous_hashing).
  fn select(hasher: Hasher, hash: Int) -> Shard {
    let mut shard = 0
    let mut max = 0
    let len = @shards.size

    for idx in 0.until(len) {
      let shard_hash = hasher.hash((idx, hash))

      if shard_hash > max {
        shard = idx
        max = shard_hash
      }
    }

    # Since we're guaranteed to always have a single shard (due to the
    # `Config` type requiring at least a single shard), this panic never gets
    # triggered.
    @shards.get(shard).or_panic
  }

  fn to_array -> Array[Shard] {
    # We can't just clone the `Array` because `Shard` is a process and processes
    # can't implement traits, so we have to clone the references manually.
    #
    # Because process references use atomic reference counting, these "clones"
    # just increment the reference count of each process.
    let val = Array.with_capacity(@shards.size)

    for shard in @shards.iter { val.push(shard) }

    val
  }
}

impl Clone for Shards {
  fn pub clone -> Self {
    Shards(to_array)
  }
}

# A process that stores a list of key-value pairs.
#
# Rather than storing data in a single place, each Shard owns a chunk of
# key-value pairs. The assignment of pairs to shards is determined using
# rendezvous hashing. This approach allows for concurrent access to different
# key-value pairs.
type async Shard {
  # The key-value pairs stored in this shard.
  #
  # We only support a few basic operations on string key/value pairs, so we only
  # need a single mapping. In a real database we'd likely have different data
  # structures for different types of keys.
  let @keys: Map[ByteArray, ByteArray]

  fn static new -> Self {
    Shard(keys: recover Map.new)
  }

  # Assigns a key a new value.
  #
  # The `connection` argument is the connection that requested the operation,
  # and will be resumed upon success.
  #
  # The `key` argument is the key (name) as produced by the connection.
  #
  # The `stream` argument is used for reading and writing RESP messages.
  fn async mut set(
    connection: Connection,
    key: uni Key[ByteArray],
    stream: uni Stream[TcpClient],
  ) {
    match stream.read_bulk_string_value {
      case Ok(v) -> @keys.set(recover key, v)
      case Error(e) -> return connection.error(stream, e)
    }

    match stream.write_ok {
      case Ok(_) -> connection.resume(stream)
      case Error(e) -> connection.error(stream, e)
    }
  }

  # Retrieves the value of a key.
  #
  # The `connection` argument is the connection that requested the operation,
  # and will be resumed upon success.
  #
  # The `key` argument is the key (name) as produced by the connection.
  #
  # The `stream` argument is used for reading and writing RESP messages.
  fn async mut get(
    connection: Connection,
    key: uni Key[ByteArray],
    stream: uni Stream[TcpClient],
  ) {
    let res = match @keys.get(recover key) {
      case Some(v) -> stream.write_bulk_string(v)
      case _ -> stream.write_nil
    }

    match res {
      case Ok(_) -> connection.resume(stream)
      case Error(e) -> connection.error(stream, e)
    }
  }

  # Deletes a key-value pair.
  #
  # The `connection` argument is the connection that requested the operation,
  # and will be resumed upon success.
  #
  # The `key` argument is the key (name) as produced by the connection.
  #
  # The `stream` argument is used for reading and writing RESP messages.
  fn async mut delete(
    connection: Connection,
    key: uni Key[ByteArray],
    stream: uni Stream[TcpClient],
  ) {
    let num = @keys.remove(recover key).some?.to_int

    match stream.write_int(num) {
      case Ok(_) -> connection.resume(stream)
      case Error(e) -> connection.error(stream, e)
    }
  }

  # Collects the keys of the current shrad into the `keys` array.
  #
  # Once all keys are collected, this method schedules the next shard in the
  # list for processing. Once all shards finish collecting their keys, the
  # result is sent back to the `Connection` process.
  #
  # The `connection` argument is the connection that requested the operation,
  # and will be resumed upon collecting all keys across all shards.
  #
  # The `stream` argument is used for reading and writing RESP messages.
  #
  # The `shards` argument is an array of _remaining_ shards for which to collect
  # keys.
  #
  # The `keys` argument is an array of all the keys across all shards.
  fn async keys(
    connection: Connection,
    stream: uni Stream[TcpClient],
    shards: uni Array[Shard],
    keys: uni Array[ByteArray],
  ) {
    for key in @keys.keys { keys.push(recover key.clone) }

    match shards.pop {
      case Some(s) -> s.keys(connection, stream, shards, keys)
      case _ -> connection.write_keys(stream, keys)
    }
  }
}
